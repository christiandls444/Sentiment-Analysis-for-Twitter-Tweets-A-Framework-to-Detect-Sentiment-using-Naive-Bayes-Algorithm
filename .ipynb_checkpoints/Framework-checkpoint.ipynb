{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9db5665",
   "metadata": {},
   "source": [
    "# Modeling and Analysis: Implementation of NLP and ML Approach for Sentiment Detection on Twitter Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6348e72",
   "metadata": {},
   "source": [
    "### FETCH DATA FROM TWITTER USING TWITTER API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a054db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "my_api_key = \"nZA1UP3eIW9KbRPOAXQZcN7nH\"\n",
    "my_api_secret = \"S0KjwqFMo7RJAtoNBCcDjToHmYXfjXRVsni0VDJpv8fyEo48sf\"\n",
    "auth = tw.OAuthHandler(my_api_key, my_api_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d1c50a",
   "metadata": {},
   "source": [
    "### TOPIC EXPERIMENT WHAT SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96668d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = \"@bongbongmarcos\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b2b74",
   "metadata": {},
   "source": [
    "### SET HOW MANY DATA WILL BE COLLECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffacbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tw.Cursor(api.search, q=search_query, lang=\"en\", since=\"2021-01-01\").items(5000)\n",
    "tweet_copy = []\n",
    "for tweet in tweet:\n",
    "    tweet_copy.append(tweet)    \n",
    "print(\"Total Tweets fetched:\", len(tweet_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd68db",
   "metadata": {},
   "source": [
    "### DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab5822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "data = pd.DataFrame()\n",
    "for tweet in tweet_copy:\n",
    "    hashtags = []\n",
    "    try:\n",
    "        for hashtag in tweet.entities[\"hashtags\"]:\n",
    "            hashtags.append(hashtag[\"text\"])\n",
    "        text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "    except:\n",
    "        pass\n",
    "    data = data.append(pd.DataFrame({'user_name': tweet.user.name, 'date': tweet.created_at,'text': text, 'hashtags': [hashtags if hashtags else None],'source': tweet.source}))\n",
    "    data = data.reset_index(drop=True)\n",
    "data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e460d1",
   "metadata": {},
   "source": [
    "### DESCRIBING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24474b",
   "metadata": {},
   "source": [
    "DATA TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85915c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceda498",
   "metadata": {},
   "source": [
    "COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675c009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb97dfe",
   "metadata": {},
   "source": [
    "INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6773a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d951cc",
   "metadata": {},
   "source": [
    "SHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b829b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9135dd41",
   "metadata": {},
   "source": [
    "SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29194a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf6f736",
   "metadata": {},
   "source": [
    "DROP COLUMN HASHTAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08595bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f517e1c",
   "metadata": {},
   "source": [
    "CHECK SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e2dd8",
   "metadata": {},
   "source": [
    "DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d01cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71133994",
   "metadata": {},
   "source": [
    "### BASIC TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa43df3",
   "metadata": {},
   "source": [
    "SET TEXT LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cea7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "data['lc'] = data['text'].str.lower()\n",
    "data[['text', 'lc']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34219f",
   "metadata": {},
   "source": [
    "REMOVE UNWANTED SPACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data['us'] = data['lc'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "data[['lc', 'us']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34bb43",
   "metadata": {},
   "source": [
    "REMOVE URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['url'] = data['us'].apply(lambda x:re.sub(r\"http\\S+|https\\S+\", \"\", x, flags = re.MULTILINE))\n",
    "data[['us', 'url']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139868d7",
   "metadata": {},
   "source": [
    "REMOVE HTML ENCONDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0c0c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['html'] = data['url'].apply(lambda x: re.sub('<[^<]+?>', '', x))\n",
    "data[['url', 'html']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b4b3e",
   "metadata": {},
   "source": [
    "REMOVE @USERNAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['un'] = data['html'].apply(lambda x: re.sub('@[A-Za-z0-9]+', '', x))\n",
    "data[['html', 'un']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629da8b0",
   "metadata": {},
   "source": [
    "REMOVE PUNCTUATIONS, NUMBERS, EMOJIS, ETC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pne'] = data['un'].apply(lambda x: re.sub('[^A-Za-z]', ' ', x))\n",
    "data[['un', 'pne']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f8f1f",
   "metadata": {},
   "source": [
    "REMOVE 3 AND LESS CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9f9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['two'] = data['pne'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "data[['pne', 'two']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb969816",
   "metadata": {},
   "source": [
    "### INTERMEDIATE TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ebc02",
   "metadata": {},
   "source": [
    "REMOVE STOPWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfad7fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def def_sw(texts):    \n",
    "    tweet_tokens = word_tokenize(texts)\n",
    "    sw = [word for word in tweet_tokens if word not in stop]        \n",
    "    return \" \".join(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d6e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sw'] = data['two'].apply(lambda x: def_sw(x))\n",
    "data[['two', 'sw']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa767e60",
   "metadata": {},
   "source": [
    "LEMMATIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f2b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "def def_lt(tweets):     \n",
    "    tweet_tokens = word_tokenize(tweets)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos = 'a') for w in tweet_tokens] #adjective\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos = 'n') for w in lemma_words]  #nouns\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos = 'r') for w in lemma_words]  #adverb\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos = 'v') for w in lemma_words]  #verb  \n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab5127",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lt'] = data['sw'].apply(lambda x: def_lt(x))\n",
    "data[['sw', 'lt']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe32821",
   "metadata": {},
   "source": [
    "REMOVE DUPLICATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "data['dc'] = (data['lt'].str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' '))\n",
    "data[['lt', 'dc']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293b79c",
   "metadata": {},
   "source": [
    "CLEANED TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c482b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'] =  data['dc']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18cded",
   "metadata": {},
   "source": [
    "### SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfc166",
   "metadata": {},
   "source": [
    "VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5080e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data['compound'] = [analyzer.polarity_scores(v)['compound']*100 for v in data['lt']]\n",
    "data['neg'] = [analyzer.polarity_scores(v)['neg']*100 for v in data['lt']]\n",
    "data['neu'] = [analyzer.polarity_scores(v)['neu']*100 for v in data['lt']]\n",
    "data['pos'] = [analyzer.polarity_scores(v)['pos']*100 for v in data['lt']]\n",
    "data[['cleaned_text', 'neg', 'neu', 'pos', 'compound']].round(2).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ffad7",
   "metadata": {},
   "source": [
    "SENTIMENT LABELED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244b2e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Analysis(score): \n",
    "    if score >= 0.05:\n",
    "        return 1\n",
    "    elif score <= -0.05:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "data[\"st\"] = data[\"compound\"].apply(Analysis)\n",
    "data[['cleaned_text', 'st']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f9137",
   "metadata": {},
   "source": [
    "SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5204c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.countplot(data.st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['st'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a01433",
   "metadata": {},
   "source": [
    "MOST FREQUENTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf08599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cbs = '#75f0bd'\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "words = cv.fit_transform(data.cleaned_text)\n",
    "sum_words = words.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
    "frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = cbs)\n",
    "plt.title(\"Most Frequently Occuring Words - Top 30\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa66c7",
   "metadata": {},
   "source": [
    "### BY WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e957b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad3d3f",
   "metadata": {},
   "source": [
    "VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871c11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "words_df['compound'] = [analyzer.polarity_scores(v)['compound']*100 for v in words_df['word']]\n",
    "words_df['neg'] = [analyzer.polarity_scores(v)['neg']*100 for v in words_df['word']]\n",
    "words_df['neu'] = [analyzer.polarity_scores(v)['neu']*100 for v in words_df['word']]\n",
    "words_df['pos'] = [analyzer.polarity_scores(v)['pos']*100 for v in words_df['word']]\n",
    "words_df[['word', 'neg', 'neu', 'pos', 'compound']].round(2).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01416155",
   "metadata": {},
   "source": [
    "SENTIMENT LABELED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c0d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_analysis(score): \n",
    "    if score >= 0.05:\n",
    "        return 1\n",
    "    elif score <= -0.05:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2\n",
    "words_df[\"labeled\"] = words_df[\"compound\"].apply(tweet_analysis)\n",
    "words_df[['word', 'labeled']].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df57341e",
   "metadata": {},
   "source": [
    "SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.countplot(words_df.labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df['labeled'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c0590",
   "metadata": {},
   "source": [
    "FIRST 10 LARGEST POSITIVE VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df[['word', 'pos', 'compound']].nlargest(10, ['pos']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f9c5c3",
   "metadata": {},
   "source": [
    "FIRST 10 LARGEST NEGATIVE VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff2a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df[['word', 'neg', 'compound']].nlargest(10, ['neg']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a4c68",
   "metadata": {},
   "source": [
    "WORDCLOUD POSITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "from matplotlib import colors\n",
    "color_list=['#262D35','#ffcc00','#000002',]\n",
    "colormap=colors.ListedColormap(color_list)\n",
    "words = ' '.join([Text for Text in words_df[words_df['labeled']==1]['word']])\n",
    "wordCloud = WordCloud(background_color='white',colormap=colormap, mode=\"RGB\", width=2000 , height=1000).generate(words)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordCloud)\n",
    "plt.title(\"Wordcloud Positive\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342de17f",
   "metadata": {},
   "source": [
    "### WORCLOUD NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c75f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "from matplotlib import colors\n",
    "from nltk.corpus import stopwords\n",
    "color_list=['#262D35','#ffcc00','#000002',]\n",
    "colormap=colors.ListedColormap(color_list)\n",
    "words = ' '.join([Text for Text in words_df[words_df['labeled']==0]['word']])\n",
    "wordCloud = WordCloud(background_color='white',colormap=colormap, mode=\"RGB\", width=2000 , height=1000).generate(words)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordCloud)\n",
    "plt.title(\"Wordcloud Negative\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7914c46a",
   "metadata": {},
   "source": [
    "TEXTBLOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "def get_sub(Tweets):\n",
    "    return TextBlob(Tweets).sentiment.subjectivity\n",
    "def get_pol(Tweets):\n",
    "    return TextBlob(Tweets).sentiment.polarity\n",
    "words_df['subjectivity'] = words_df['word'].apply(get_sub)\n",
    "words_df['polarity'] = words_df['word'].apply(get_pol)\n",
    "words_df[['word', 'subjectivity', 'polarity','labeled']].head(7)#.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea26ba8",
   "metadata": {},
   "source": [
    "SUBJECTIVITY VS OBJECTIVITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ab2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = '#75f0bd'\n",
    "import numpy\n",
    "plt.figure(figsize=(8,6))\n",
    "for i in range(0, words_df.shape[0]):\n",
    "    plt.scatter(words_df['polarity'][i], words_df['subjectivity'][i], color=cbs)   \n",
    "plt.title('Scatter plot')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7451d2",
   "metadata": {},
   "source": [
    "### MACHINE LEARNING APROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93321df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "dataset.iloc[5050:5057]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d76f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6b0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def preprocess(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    tweet = re.sub(r\"http\\S+|https\\S+\", \"\", tweet, flags = re.MULTILINE)\n",
    "    tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "    tweet = re.sub('@[A-Za-z0-9]+', '', tweet)\n",
    "    tweet = re.sub(\"[^a-zA-Z]\", ' ', tweet)\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [word for word in tweet_tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos = 'a') for w in filtered_words] #adjective\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos = 'n') for w in lemma_words]  #nouns\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos = 'r') for w in lemma_words]  #adverb\n",
    "    lemma_words = [lemmatizer.lemmatize(w, pos = 'v') for w in lemma_words]  #verb  \n",
    "    return \" \".join(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228e443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "dataset['pr'] = dataset['tweet'].apply(lambda x: preprocess(x))\n",
    "dataset['two'] = dataset['pr'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "dataset['dc'] = (dataset['two'].str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' '))\n",
    "dataset[['tweet', 'dc']].iloc[5050:5057]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['ct'] = dataset['dc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "corpus = []\n",
    "for i in range(0, len(dataset)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['ct'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "bow_transformer = CountVectorizer(stop_words='english')\n",
    "bow_transformer = bow_transformer.fit(corpus)\n",
    "print('Length of the Vocabulary: ',len(bow_transformer.vocabulary_))\n",
    "messages_bow = bow_transformer.transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "X = tfidf_transformer.transform(messages_bow)\n",
    "y = []\n",
    "for row in dataset[\"labeled\"]:\n",
    "    y.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d28911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=bow_transformer.get_feature_names(),columns=[\"idf_weights\"])\n",
    "df_idf.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071d59f",
   "metadata": {},
   "source": [
    "TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744de568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67c3e2",
   "metadata": {},
   "source": [
    "IMPLEMENT MULTINOMIAL NAIVE BAYES MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7725f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes  import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred_classifier = classifier.predict(X_test)\n",
    "print('Naive Bayes Results:')\n",
    "print(classification_report(y_test, y_pred_classifier))\n",
    "confusion_matrix = confusion_matrix(y_test,y_pred_classifier)\n",
    "print(\"\\nConfusion Matrix\\n\", confusion_matrix)\n",
    "plot_confusion_matrix(classifier, X_test, y_test,cmap=\"GnBu\");\n",
    "print(\"Multinomial Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred_classifier).round(2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d3e82a",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579cbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ['i love computer science']\n",
    "new_test = bow_transformer.transform(test_set)\n",
    "classifier.predict(new_test)\n",
    "# array([0]) = Negative\n",
    "# array([1]) = Positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3e8b7",
   "metadata": {},
   "source": [
    "### DETECT FROM TWITTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'].iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [data['cleaned_text'].iloc[15]]\n",
    "new_test = bow_transformer.transform(test_set)\n",
    "classifier.predict(new_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
